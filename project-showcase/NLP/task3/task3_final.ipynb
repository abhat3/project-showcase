{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "task3_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPtzCdu8DBfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "from sklearn import utils\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "from textblob import TextBlob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFEWorTUDBfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('task3_all.tsv', sep=\"\\t\", header=None)\n",
        "train.columns = train.iloc[0]\n",
        "train = train.drop(train.index[0])\n",
        "train = train.astype(str)\n",
        "\n",
        "test = pd.read_csv('task3_validation.tsv', sep=\"\\t\", header=None)\n",
        "test.columns = test.iloc[0]\n",
        "test = test.drop(test.index[0])\n",
        "test = test.astype(str)\n",
        "\n",
        "train = train[train.type == 'ADR']\n",
        "test = test[test.type == 'ADR']\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def cleanText(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text\n",
        "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
        "    text = text.lower()\n",
        "    return text\n",
        "train['extraction'] = train['extraction'].apply(cleanText)\n",
        "test['extraction'] = test['extraction'].apply(cleanText)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcZIkGPRDBfj",
        "colab_type": "code",
        "colab": {},
        "outputId": "653a6cd0-23d6-45ec-bb72-7352117917fc"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>begin</th>\n",
              "      <th>end</th>\n",
              "      <th>type</th>\n",
              "      <th>extraction</th>\n",
              "      <th>drug</th>\n",
              "      <th>tweet</th>\n",
              "      <th>meddra_code</th>\n",
              "      <th>meddra_term</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>332317478170546176</td>\n",
              "      <td>28</td>\n",
              "      <td>37</td>\n",
              "      <td>ADR</td>\n",
              "      <td>allergies</td>\n",
              "      <td>avelox</td>\n",
              "      <td>do you have any medication allergies? \"asthma!...</td>\n",
              "      <td>10013661</td>\n",
              "      <td>drug allergy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>347806215776116737</td>\n",
              "      <td>31</td>\n",
              "      <td>46</td>\n",
              "      <td>ADR</td>\n",
              "      <td>hurt your liver</td>\n",
              "      <td>avelox</td>\n",
              "      <td>@ashleylvivian if #avelox has hurt your liver,...</td>\n",
              "      <td>10024668</td>\n",
              "      <td>liver damage</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>350336129817509888</td>\n",
              "      <td>48</td>\n",
              "      <td>50</td>\n",
              "      <td>ADR</td>\n",
              "      <td>ad</td>\n",
              "      <td>baclofen</td>\n",
              "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
              "      <td>10003731</td>\n",
              "      <td>attention deficit disorder</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>350336129817509888</td>\n",
              "      <td>88</td>\n",
              "      <td>93</td>\n",
              "      <td>ADR</td>\n",
              "      <td>focus</td>\n",
              "      <td>baclofen</td>\n",
              "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
              "      <td>10003738</td>\n",
              "      <td>attention impaired</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>332540699692130304</td>\n",
              "      <td>11</td>\n",
              "      <td>15</td>\n",
              "      <td>ADR</td>\n",
              "      <td>died</td>\n",
              "      <td>cipro</td>\n",
              "      <td>pt of mine died from cipro rt @ciproispoison: ...</td>\n",
              "      <td>10011906</td>\n",
              "      <td>death</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "0            tweet_id begin end type       extraction      drug  \\\n",
              "1  332317478170546176    28  37  ADR        allergies    avelox   \n",
              "2  347806215776116737    31  46  ADR  hurt your liver    avelox   \n",
              "3  350336129817509888    48  50  ADR               ad  baclofen   \n",
              "4  350336129817509888    88  93  ADR            focus  baclofen   \n",
              "5  332540699692130304    11  15  ADR             died     cipro   \n",
              "\n",
              "0                                              tweet meddra_code  \\\n",
              "1  do you have any medication allergies? \"asthma!...    10013661   \n",
              "2  @ashleylvivian if #avelox has hurt your liver,...    10024668   \n",
              "3  apparently, baclofen greatly exacerbates the \"...    10003731   \n",
              "4  apparently, baclofen greatly exacerbates the \"...    10003738   \n",
              "5  pt of mine died from cipro rt @ciproispoison: ...    10011906   \n",
              "\n",
              "0                 meddra_term  \n",
              "1                drug allergy  \n",
              "2                liver damage  \n",
              "3  attention deficit disorder  \n",
              "4          attention impaired  \n",
              "5                       death  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73Yue8kzDBfn",
        "colab_type": "code",
        "colab": {},
        "outputId": "4120d08e-27cc-4228-e24e-87214d0b22b6"
      },
      "source": [
        "train.meddra_code.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10047896    74\n",
              "10073281    65\n",
              "10048010    39\n",
              "10043890    36\n",
              "10016370    36\n",
              "10041349    35\n",
              "10019211    35\n",
              "10041001    33\n",
              "10033371    31\n",
              "10016365    30\n",
              "10022437    27\n",
              "10016384    27\n",
              "10001718    21\n",
              "10041014    20\n",
              "10001125    19\n",
              "10028813    18\n",
              "10041000    17\n",
              "10043087    15\n",
              "10012336    15\n",
              "10004969    15\n",
              "10003988    14\n",
              "10016336    14\n",
              "10027374    14\n",
              "10042661    14\n",
              "10049278    14\n",
              "10013649    14\n",
              "10012378    14\n",
              "10019158    13\n",
              "10071175    13\n",
              "10041017    12\n",
              "            ..\n",
              "10048013     1\n",
              "10064160     1\n",
              "10044124     1\n",
              "10038001     1\n",
              "10035805     1\n",
              "10020197     1\n",
              "10009696     1\n",
              "10046571     1\n",
              "10001488     1\n",
              "10038744     1\n",
              "10043498     1\n",
              "10016821     1\n",
              "10016344     1\n",
              "10040559     1\n",
              "10001639     1\n",
              "10065015     1\n",
              "10044573     1\n",
              "10077275     1\n",
              "10028322     1\n",
              "10049183     1\n",
              "10017375     1\n",
              "10033434     1\n",
              "10042076     1\n",
              "10041005     1\n",
              "10013082     1\n",
              "10036507     1\n",
              "10044698     1\n",
              "10012790     1\n",
              "10043439     1\n",
              "10008477     1\n",
              "Name: meddra_code, Length: 475, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmhOYfqkDBfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "def tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for word in nltk.word_tokenize(sent):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word.lower())\n",
        "    return tokens\n",
        "\n",
        "train_tagged = train.apply(\n",
        "    lambda r: TaggedDocument(words=tokenize_text(r['extraction']), tags=[r.meddra_code]), axis=1)\n",
        "test_tagged = test.apply(\n",
        "    lambda r: TaggedDocument(words=tokenize_text(r['extraction']), tags=[r.meddra_code]), axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhD0DH-TDBft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5hOLKH3DBfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vec_for_learning(model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "    return targets, regressors\n",
        "\n",
        "logreg = LogisticRegression(n_jobs=1, C=1e5, max_iter=100, class_weight = 'balanced')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfMhYsW3DBfy",
        "colab_type": "text"
      },
      "source": [
        " # Distributed Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo3qeJd-DBfy",
        "colab_type": "code",
        "colab": {},
        "outputId": "586bd569-f917-4e2f-dfd3-7a3ee5c03001"
      },
      "source": [
        "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
        "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1829/1829 [00:00<00:00, 1311123.23it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oBy4f59DBf1",
        "colab_type": "code",
        "colab": {},
        "outputId": "80e7e3a7-155d-4d64-bbb6-3fc699ea0184"
      },
      "source": [
        "%%time\n",
        "for epoch in range(30):\n",
        "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
        "    model_dmm.alpha -= 0.002\n",
        "    model_dmm.min_alpha = model_dmm.alpha"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1829/1829 [00:00<00:00, 1372094.80it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2721313.24it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2771453.04it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2681363.86it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2647129.75it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2862455.98it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 1581076.26it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2931364.93it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2354629.23it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 1943598.18it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2444672.41it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2543561.68it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2195587.30it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 1640235.62it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2705954.86it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2353906.72it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2788579.43it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2848637.96it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2795693.15it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2516032.15it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2147042.27it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2688882.59it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 1494522.11it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2323253.18it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2409353.65it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 1035134.53it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 1131973.15it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2413143.13it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 2717457.32it/s]\n",
            "100%|██████████| 1829/1829 [00:00<00:00, 1576851.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.24 s, sys: 61.3 ms, total: 1.3 s\n",
            "Wall time: 1.39 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWgngHL-DBf4",
        "colab_type": "code",
        "colab": {},
        "outputId": "7ef2cf7b-1d59-4099-80e5-02f2472d5eda"
      },
      "source": [
        "y_train, X_train = vec_for_learning(model_dmm, train_tagged)\n",
        "y_test, X_test = vec_for_learning(model_dmm, test_tagged)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_test)\n",
        "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing accuracy 0.5013698630136987\n",
            "Testing F1 score: 0.5059975837574548\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/nikitasawant/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k0CvCinDBf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}