{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task2_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LL9p0a-pPJ9",
        "colab_type": "code",
        "outputId": "44828f03-571b-4cc8-ff4b-a5f761d47581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-iQu1nFbLge",
        "colab_type": "code",
        "outputId": "43138c68-b1c8-4e77-9600-639a7105a6c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "pip install contractions fasttext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/85/41/c3dfd5feb91a8d587ed1a59f553f07c05f95ad4e5d00ab78702fbf8fe48a/contractions-0.0.24-py2.py3-none-any.whl\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.0MB/s \n",
            "\u001b[?25hCollecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (46.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.4)\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 8.1MB/s \n",
            "\u001b[?25hCollecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 14.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fasttext, pyahocorasick\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3017106 sha256=6bb13fab94f9f2fcdc4a45535a0f5458ee178ab6e8a1adf41fc875b5e02f5a6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81700 sha256=c55d45cfcc27e99288f2c2efca5683dd8c808a843d25f8d892dd94b0e8a419c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built fasttext pyahocorasick\n",
            "Installing collected packages: pyahocorasick, Unidecode, textsearch, contractions, fasttext\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.24 fasttext-0.9.2 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNw0xj5fpWi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "from collections import Counter, OrderedDict\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from contractions import contractions_dict\n",
        "import unicodedata\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy import displacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "import fasttext\n",
        "from gensim.models.fasttext import FastText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PADryocSl8dH",
        "colab_type": "code",
        "outputId": "bdd82fbe-8d99-40da-d9ba-09a137c0169b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext import data as data_t\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff249aa1cb0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stlSo61GZAQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "COLUMN_LIST = [\"tweet_id\", \"begin\", \"end\", \"type\", \"extraction\", \"drug\", \"tweet\", \"meddra_code\", \"meddra_term\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dezyoJzgbwjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TWEET_COL = \"tweet\"\n",
        "TWEET_ID = \"tweet_id\"\n",
        "BEGIN_COL = \"begin\"\n",
        "END_COL = \"end\"\n",
        "TYPE_COL = \"type\"\n",
        "EXTRACTION_COL = \"extraction\"\n",
        "DRUG_COL = \"drug\"\n",
        "MEDDRA_CODE = \"meddra_code\"\n",
        "MEDDRA_TERM = \"meddra_term\"\n",
        "NON_ADR = \"NO-ADR\"\n",
        "NO_DRUG = \"NO-DRUG\"\n",
        "DRUG_TAG = \"D\"\n",
        "ADR_TAG = \"A\"\n",
        "OTHER_TAG = \"O\"\n",
        "PAD_TOKEN = \"PAD_T\"\n",
        "START_TAG = \"START-T\"\n",
        "STOP_TAG = \"STOP-T\"\n",
        "UNKNOWN = \"UNK-T\"\n",
        "BLANK_SPACE = \" \""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46QV8bl8kKEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_model = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqKGaWz9bpR0",
        "colab_type": "code",
        "outputId": "7a512a04-e1a0-4458-877c-5f21cf04c213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# get stopwords from nltk\n",
        "nltk.download('stopwords')\n",
        "NLTK_STOP = list(set(stopwords.words(\"english\")))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr7y2Uqgpjkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the training set\n",
        "data = pd.read_csv(\"/content/gdrive/My Drive/task3_training.tsv\", sep=\"\\t\", usecols=COLUMN_LIST)\n",
        "data_val = pd.read_csv(\"/content/gdrive/My Drive/task3_validation.tsv\", sep=\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KfLlD2SE4vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accented char conversion, use if english\n",
        "def removeAccented(tweet):\n",
        "  return unicodedata.normalize('NFKD', tweet).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1B0Q9jwFvKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# expand all contractions in given sentence\n",
        "def expandContr(tweet, c_dict=contractions_dict):\n",
        "  word_list = tweet.split()\n",
        "  expanded_tweet = []\n",
        "  for word in word_list:\n",
        "    if word in c_dict:\n",
        "      expanded_tweet.append(c_dict[word])\n",
        "    else:\n",
        "      expanded_tweet.append(word)\n",
        "  return BLANK_SPACE.join(expanded_tweet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmri9kVyRJML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HANDLE_MENTION = True\n",
        "HANDLE_HASHTAG = True\n",
        "REMOVE_DIGITS = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWBopHe3g53q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# custom punctuation removal \n",
        "def removePunct(tweet):\n",
        "  tweet = str(tweet)\n",
        "  word_list = tweet.split()\n",
        "  clean_tweet = []\n",
        "  for word in word_list:\n",
        "    # if it is a mention, replace it with a proper noun \n",
        "    if HANDLE_MENTION:\n",
        "      if word[0] == \"@\":\n",
        "        clean_tweet.append(word[1:])\n",
        "        continue\n",
        "    # just replace the hashtag with it's word counterpart \n",
        "    if HANDLE_HASHTAG:\n",
        "      if word[0] == \"#\":\n",
        "        clean_tweet.append(word[1:])\n",
        "        continue\n",
        "    # replace punctuations with space and build tweet cleanly\n",
        "    final_word = \"\"\n",
        "    for w in word:\n",
        "      if not REMOVE_DIGITS:\n",
        "        if w.isalnum():\n",
        "          final_word += w\n",
        "      else:\n",
        "        if w.isalpha():\n",
        "          final_word += w\n",
        "    clean_tweet.append(final_word)\n",
        "    \n",
        "  return BLANK_SPACE.join(clean_tweet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1spQ9De3Q29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# perform lemmatization\n",
        "def lemmatize(tweet):\n",
        "  tweet = spacy_model(tweet) # redundant, find a way to get over this!\n",
        "  return BLANK_SPACE.join([tok.lemma_ if tok.lemma_ != \"-PRON-\" else tok.text for tok in tweet])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_feF1uPb9SVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = 30\n",
        "USE_NLTK = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC28rdtSVNih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate top N custom stopwords \n",
        "def generateCustomStop(data):\n",
        "  custom_stopwords = []\n",
        "  count_words = Counter()\n",
        "  if USE_NLTK:\n",
        "    stop_list = NLTK_STOP\n",
        "  else:\n",
        "    stop_list = STOP_WORDS\n",
        "  for tweet in data[TWEET_COL]:\n",
        "    for word in tweet.split():\n",
        "      count_words[word] += 1\n",
        "  count = 0\n",
        "  # reverse it because \n",
        "  count_words = count_words.most_common()[::-1]\n",
        "  while count != N:\n",
        "    word = count_words.pop()[0]\n",
        "    if word in stop_list:\n",
        "      custom_stopwords.append(word)\n",
        "      count += 1\n",
        "    else:\n",
        "      continue  \n",
        "  return custom_stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXFFZj5fTU2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "R_ACC = True\n",
        "CONTR = True\n",
        "R_URL = True\n",
        "R_PUNCT = True\n",
        "LEMMATIZE = True\n",
        "R_STOP = True\n",
        "CUSTOM_STOP = True\n",
        "if CUSTOM_STOP:\n",
        "  custom_stopwords = generateCustomStop(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke3PMx7I8xEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean up\n",
        "def cleanTweet(tweet):\n",
        "  # convert to lower case \n",
        "  tweet = tweet.lower()\n",
        "  \n",
        "  # remove accented characters if any\n",
        "  if R_ACC:\n",
        "    tweet = removeAccented(tweet)\n",
        "\n",
        "  # expand contractions\n",
        "  if CONTR:\n",
        "    tweet = expandContr(tweet)\n",
        "\n",
        "  # remove urls\n",
        "  if R_URL:\n",
        "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
        "  \n",
        "  # remove punctuations\n",
        "  if R_PUNCT:\n",
        "    tweet = removePunct(tweet)\n",
        "  \n",
        "  # do lemmatization\n",
        "  if LEMMATIZE:\n",
        "    tweet = lemmatize(tweet)\n",
        "\n",
        "  # remove stopwords\n",
        "  if R_STOP:\n",
        "    if CUSTOM_STOP:\n",
        "      tweet = BLANK_SPACE.join([i for i in tweet.split() if i not in custom_stopwords])\n",
        "    else:\n",
        "      tweet = BLANK_SPACE.join([i for i in tweet.split() if i not in STOP_WORDS])\n",
        "  # tweet = BLANK_SPACE.join([dict_drugs[i] if i in dict_drugs else i for i in tweet.split()])\n",
        "  tweet = BLANK_SPACE.join([i for i in tweet.split() if not len(i) == 0])\n",
        "  return tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFGqViSiRkwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanData(data, column_name):\n",
        "  clean_data = data.copy()\n",
        "  clean_data[column_name] = data[column_name].apply(cleanTweet)\n",
        "  return clean_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iSBxWUBYnZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getColumnAsList(data, column_name): \n",
        "  return list(data[column_name])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa0kx-TQrMd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[EXTRACTION_COL].fillna(value=NON_ADR, inplace=True)\n",
        "data[DRUG_COL].fillna(value=NO_DRUG, inplace=True)\n",
        "data[DRUG_COL] = data[DRUG_COL].apply(str.lower)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13l1Ye9Zhz9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean the data\n",
        "data = cleanData(data, TWEET_COL)\n",
        "data = cleanData(data, EXTRACTION_COL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzygdstmUCeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.drop_duplicates([TWEET_COL])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtk32UYPDF4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data_overload = cleanData(data_overload, TWEET_COL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MynNOWP8je7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get tag to vector\n",
        "# tag_list = [ADR_TAG, DRUG_TAG, OTHER_TAG, START_TAG, STOP_TAG]\n",
        "tag_list = [ADR_TAG, OTHER_TAG, START_TAG, STOP_TAG]\n",
        "tag_to_vector = {tag:idx for idx, tag in enumerate(tag_list)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqazF1Tnjk5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get word list\n",
        "def getWordNERList(data):\n",
        "  PAD_LENGTH = 36\n",
        "  PAD_TAG = \"_pad_\"\n",
        "  word_ner = []\n",
        "  words_list = []\n",
        "  sentence_list = []\n",
        "  sentence_words = []\n",
        "  tag_list = []\n",
        "  for idx, row in data.iterrows():\n",
        "    sentence_word = []\n",
        "    sentence_tag = []\n",
        "    sentence_list.append(row[TWEET_COL])\n",
        "    for word in row[TWEET_COL].split(\" \"):\n",
        "      words_list.append(word)\n",
        "      # if word == row[DRUG_COL]:\n",
        "      #   sentence_word.append(word)\n",
        "      #   sentence_tag.append(DRUG_TAG)\n",
        "      #   tag_list.append(DRUG_TAG)\n",
        "      # elif word in row[EXTRACTION_COL].split(\" \"):\n",
        "      if word in row[EXTRACTION_COL].split(\" \"):\n",
        "        sentence_word.append(word)\n",
        "        sentence_tag.append(ADR_TAG)\n",
        "        tag_list.append(ADR_TAG)\n",
        "      else:\n",
        "        sentence_word.append(word)\n",
        "        sentence_tag.append(OTHER_TAG)\n",
        "        tag_list.append(OTHER_TAG)\n",
        "    # sentence_words.append(BLANK_SPACE.join(sentence_word))\n",
        "    word_ner.append((sentence_word, sentence_tag))\n",
        "  return word_ner, words_list, sentence_list, tag_list, words_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwnCxalRkyEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_tags, all_words, all_sentences, all_tags, all_sentence_words = getWordNERList(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfhrJ-pd5OQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write sentences to file\n",
        "with open('/content/gdrive/My Drive/model_sentences.txt', 'w') as filehandle:\n",
        "    for listitem in all_sentences:\n",
        "        filehandle.write('%s\\n' % listitem)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "020I9XmxpdVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(all_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sVA1n2imf_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getWordVecs(data):\n",
        "  word_to_ix = {}\n",
        "  word_to_ix[UNKNOWN] = len(word_to_ix)\n",
        "  for word in data:\n",
        "      if word not in word_to_ix:\n",
        "          word_to_ix[word] = len(word_to_ix)\n",
        "  return word_to_ix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WfKkP3zlIqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_ix = getWordVecs(all_words)\n",
        "len(word_to_ix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV8OQ6YC9jWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_val[EXTRACTION_COL].fillna(value=NON_ADR, inplace=True)\n",
        "data_val[DRUG_COL].fillna(value=NO_DRUG, inplace=True)\n",
        "data_val[DRUG_COL] = data_val[DRUG_COL].apply(str.lower)\n",
        "data_val = cleanData(data_val, TWEET_COL)\n",
        "data_val = cleanData(data_val, EXTRACTION_COL)\n",
        "data_val = data_val.drop_duplicates([TWEET_COL])\n",
        "\n",
        "sentence_tags_valll, all_words_val, all_sentences_val, all_tags_val, all_sentence_words_val = getWordNERList(data_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94KwI5uIoFTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(all_words_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE1QfC9_i89S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "WINDOW_SIZE = 10\n",
        "MIN_WORD = 5\n",
        "DOWN_SAMPLING = 1e-2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtxiV_xr3c8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fasttext model\n",
        "fasttext_model = fasttext.train_unsupervised('/content/gdrive/My Drive/model_sentences.txt', model='skipgram', dim=EMBEDDING_DIM,\n",
        "                                    ws=WINDOW_SIZE, minCount=MIN_WORD, t=DOWN_SAMPLING)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgAlKwit-h-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get embeddings for each word\n",
        "tensor_list = list()\n",
        "tensor_list.append([0.0]*100)\n",
        "for word in set(all_words):\n",
        "  tensor_list.append(list(fasttext_model[word]))\n",
        "embeddings_fasttext = torch.Tensor(tensor_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elO3CvhJpSUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_fasttext.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7jVc8kAciM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def argmax(vec):\n",
        "    # return the argmax \n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "# convert word to positional vector representation\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] if w in to_ix else to_ix[UNKNOWN] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "# compute log sum exponent for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "# calc epoch time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeHvPR3Bhn4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_vector, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_vector = tag_to_vector\n",
        "        self.tagset_size = len(tag_to_vector)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        print(self.vocab_size)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning to 'i' from 'j'.\n",
        "        # CRF graph for transitions amongst different labels/states\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # So that we never transfer to the start tag backward\n",
        "        # and we never transfer from the stop tag forward\n",
        "        self.transitions.data[tag_to_vector[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_vector[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_vector[START_TAG]] = 0.\n",
        "\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence and find which feature\n",
        "        # has how much probability of changing to which state \n",
        "        # based on lstm features, general transition score\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors \n",
        "            for next_tag in range(self.tagset_size):\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # transition score \n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_vector[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        # print(self.hidden[0].size())\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        # reshape for next layer \n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_vector[START_TAG]], dtype=torch.long), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_vector[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_vector[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_vector[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag \n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_vector[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence): \n",
        "        # print(\"Forward called!\")\n",
        "        # Get the  scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r9L21uViQyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_vector, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "model.word_embeds.weight.data.copy_(embeddings_fasttext)\n",
        "loss_threshold = 0.1\n",
        "# Check predictions before training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(sentence_tags[0][0], word_to_ix)\n",
        "    precheck_tags = torch.tensor([tag_to_vector[t] for t in sentence_tags[0][1]], dtype=torch.long)\n",
        "    print(model(precheck_sent))\n",
        "\n",
        "for epoch in range(100):  \n",
        "    start_time = time.time()\n",
        "\n",
        "    for sentence, tags in sentence_tags:\n",
        "        # Pytorch accumulates gradients.\n",
        "        model.zero_grad()\n",
        "\n",
        "        # create tensor of sentences \n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = torch.tensor([tag_to_vector[t] for t in tags], dtype=torch.long)\n",
        "\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # calc time to each epoch\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    print(\"Loss:\", loss)\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    if loss < loss_threshold:\n",
        "      break\n",
        "# Check predictions after training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(sentence_tags[0][0], word_to_ix)\n",
        "    print(model(precheck_sent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPwuqheIp0ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEXT.vocab.vectors.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bI80kDL96Mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_sentence_tags, val_words, val_sentences, tag_val_all, words_val_all = getWordNERList(data_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqlZHhuOCUQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_words_sent = [[word for word in sentence.split()] for sentence in val_sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGzMEukfC5BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_original_tags = [[tag_to_vector[tag] for tag in sentence[1]] for sentence in val_sentence_tags]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7ufCS8sUyYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LENGTH_VAL = len(val_sentence_tags)\n",
        "print(len(tag_val_all))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jir0uQieswrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(all_original_tags[0])\n",
        "print(sentence_tags[0][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYAYe4vPCrnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prediction\n",
        "compare_list = list()\n",
        "total = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for idx in range(LENGTH_VAL):\n",
        "    compare_list.append((model(prepare_sequence(val_sentence_tags[idx][0], word_to_ix)), all_original_tags[idx]))\n",
        "    total += len(val_sentences[idx].split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TPnb057c_vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score, precision_score, plot_precision_recall_curve, recall_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA4kWpq9cEPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = [pred[1] for pred, orig in compare_list]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU3mar9mpDbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# flatten all tags\n",
        "final_pred = []\n",
        "final_true = []\n",
        "for orig, pred in zip(all_original_tags, predictions):\n",
        "  for t, p in zip(orig, pred):\n",
        "    final_true.append(t)\n",
        "    final_pred.append(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWcl1-9sdBHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Final metric: \")\n",
        "print(\"F1-score:\", f1_score(final_true, final_pred, zero_division=1, average='macro'))\n",
        "print(\"Precision:\", precision_score(final_true, final_pred, zero_division=1, average='macro'))\n",
        "print(\"Recall:\", recall_score(final_true, final_pred, zero_division=1, average='macro'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njeO7BH65RP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compare_length = len(compare_list)\n",
        "# f1_acc = 0\n",
        "# prec_acc = 0\n",
        "# recall_acc = 0\n",
        "\n",
        "# # calculating accuracy for each sentence and averaging it \n",
        "# for pred, y in compare_list:\n",
        "#   f1_acc += f1_score(y, pred[1], zero_division=1, average='macro')\n",
        "#   prec_acc += precision_score(y, pred[1], zero_division=1, average='macro')\n",
        "#   recall_acc += recall_score(y, pred[1], zero_division=1, average='macro')\n",
        "# print(f1_acc/compare_length)\n",
        "# print(prec_acc/compare_length)\n",
        "# print(recall_acc/compare_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVnLBpt4oAoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}