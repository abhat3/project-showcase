{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Fashion MNIST dataset\n",
    "\n",
    "import util_mnist_reader\n",
    "X_train, y_train = util_mnist_reader.load_mnist('../data/fashion', kind='train')\n",
    "X_test, y_test = util_mnist_reader.load_mnist('../data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use train_test_split to extract a small portion of training data for validation as well\n",
    "# X_train, X_val, y_train, y_val = model_selection.train_test_split(X_train, y_train, test_size = 0.05, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    def __init__(self, input_columns, num_hidden, num_classes):\n",
    "        self.W1 = np.random.randn(input_columns, num_hidden)\n",
    "        self.b1 = np.random.randn(1, num_hidden)\n",
    "        self.W2 = np.random.randn(num_hidden, num_classes)\n",
    "        self.b2 = np.random.randn(1, num_classes)\n",
    "    \n",
    "    def sigmoid(self, X):\n",
    "        return (1 / (1 + np.exp(-X)))\n",
    "    \n",
    "    def ReLU(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def tanH(self, X):\n",
    "        pass\n",
    "    \n",
    "    def softmax(self, X):\n",
    "        exps = np.exp(X - np.max(X))\n",
    "        return exps / np.sum(exps)\n",
    "    \n",
    "    def forward_pass(self, X, y, hidden_activation):\n",
    "        hidden_activation = hidden_activation.lower()\n",
    "        z1 = np.dot(X, self.W1) + self.b1\n",
    "        if(hidden_activation == \"sigmoid\"):\n",
    "            a1 = self.sigmoid(z1)\n",
    "        elif(hidden_activation == \"relu\"):\n",
    "            a1 = self.ReLU(z1)\n",
    "        elif(hidden_activation == \"tanh\"):\n",
    "            a1 = self.tanH(z1)\n",
    "        else: \n",
    "            raise ValueError(hidden_activation)\n",
    "        z2 = np.dot(a1, self.W2) + self.b2\n",
    "        a2 = self.softmax(z2)\n",
    "        return a1, a2\n",
    "    \n",
    "    def calc_loss(self, fp_result, y):\n",
    "        num_samples = y.shape[0]\n",
    "        \n",
    "        log_likelihood = -np.log(fp_result[range(num_samples), y])\n",
    "        loss = np.sum(log_likelihood) / num_samples\n",
    "        return loss\n",
    "    \n",
    "    def backPropagation(self, X, y, activation1_result, fp_result):\n",
    "        num_samples = X.shape[0]\n",
    "        \n",
    "        fp_result[range(num_samples), y] -= 1\n",
    "        fp_result = fp_result / num_samples\n",
    "        \n",
    "        dz2 = fp_result\n",
    "        \n",
    "        dW2 = np.dot(activation1_result.T, dz2)\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        dz1 = np.dot(dz2, self.W2.T)\n",
    "        dz1[activation1_result <= 0] = 0\n",
    "        \n",
    "        dW1 = np.dot(X.T, dz1)\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "        \n",
    "    \n",
    "    def train_model(self, X, y, X_val, y_val, hidden_activation = \"relu\", epochs = 10, learning_rate = 0.01):\n",
    "        train_loss = []\n",
    "        num_samples = X.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            activation1_result, fp_result = self.forward_pass(X, y, hidden_activation)\n",
    "            train_loss.append(self.calc_loss(fp_result, y))\n",
    "            dW1, db1, dW2, db2 = self.backPropagation(X, y, activation1_result, fp_result)\n",
    "            \n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "        return train_loss\n",
    "    \n",
    "    def predict(self, X, hidden_activation = \"relu\"):\n",
    "        hidden_activation = hidden_activation.lower()\n",
    "        z1 = np.dot(X, self.W1) + self.b1\n",
    "        a1 = self.ReLU(z1)\n",
    "        z2 = np.dot(a1, self.W2) + self.b2\n",
    "        y_pred = np.argmax(z2, axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
